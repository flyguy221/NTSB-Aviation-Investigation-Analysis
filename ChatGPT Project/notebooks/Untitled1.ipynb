{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8cfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim import corpora, models\n",
    "import re\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "pathway = \"/Users/jeremyfeagan/Library/Mobile Documents/com~apple~CloudDocs/MyGitRepo/ChatGPT Project/conversations.json\"\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(pathway, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64033622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeremyfeagan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Extend STOPWORDS with common programming and mathematical terms if necessary\n",
    "extended_stopwords = STOPWORDS.union(set(['=', '+', '-', '*', '/', '(', ')', '#', '->', 'int', 'float', 'print', 'def']))\n",
    "\n",
    "# Initialize a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc5d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conversation texts from the 'mapping' key\n",
    "text_data = []\n",
    "for conversation in data:\n",
    "    for key, value in conversation['mapping'].items():\n",
    "        message = value.get('message')\n",
    "        if message:\n",
    "            content = message.get('content')\n",
    "            if content:\n",
    "                parts = content.get('parts')\n",
    "                if parts and isinstance(parts, list):  # Ensure 'parts' is a list\n",
    "                    parts_texts = []\n",
    "                    for part in parts:\n",
    "                        if isinstance(part, dict) and 'text' in part:\n",
    "                            parts_texts.append(part.get('text', ''))\n",
    "                        elif isinstance(part, str):\n",
    "                            parts_texts.append(part)\n",
    "                    combined_text = ' '.join(parts_texts).strip()\n",
    "                    if combined_text:  # Ensure non-empty string\n",
    "                        text_data.append(combined_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = [text.split() for text in text_data]\n",
    "\n",
    "# Create a dictionary and document-term matrix\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_text]\n",
    "\n",
    "# Train an LDA topic model\n",
    "lda_model = models.LdaModel(doc_term_matrix, num_topics=5, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3bef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.027*\"health\" + 0.009*\"may\" + 0.007*\"treatment\" + 0.007*\"valuebased\" + 0.007*\"data\"')\n",
      "(1, '0.015*\"healthcare\" + 0.010*\"patient\" + 0.010*\"access\" + 0.010*\"reimbursement\" + 0.010*\"patients\"')\n",
      "(2, '0.043*\"column\" + 0.038*\"nan\" + 0.023*\"length\" + 0.022*\"int\" + 0.021*\"dtype\"')\n",
      "(3, '0.016*\"model\" + 0.015*\"word\" + 0.013*\"sentence\" + 0.010*\"return\" + 0.010*\"regression\"')\n",
      "(4, '0.008*\"provide\" + 0.008*\"information\" + 0.005*\"understanding\" + 0.005*\"questions\" + 0.005*\"specific\"')\n",
      "(5, '0.017*\"item\" + 0.017*\"value\" + 0.012*\"number\" + 0.011*\"landing\" + 0.010*\"rate\"')\n",
      "(6, '0.008*\"agreement\" + 0.007*\"food\" + 0.007*\"heart\" + 0.007*\"options\" + 0.007*\"blood\"')\n",
      "(7, '0.026*\"data\" + 0.014*\"code\" + 0.012*\"file\" + 0.009*\"import\" + 0.008*\"column\"')\n",
      "(8, '0.011*\"content\" + 0.011*\"use\" + 0.009*\"users\" + 0.009*\"color\" + 0.008*\"text\"')\n",
      "(9, '0.007*\"american\" + 0.006*\"like\" + 0.006*\"literature\" + 0.005*\"one\" + 0.005*\"time\"')\n"
     ]
    }
   ],
   "source": [
    "# Custom preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove hexadecimal codes and non-ASCII characters\n",
    "    text = re.sub(r'\\b[0-9a-fA-F]{4,}\\b', '', text)\n",
    "    # Keep only words with alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Assuming 'text_data' is a list of your conversation texts\n",
    "processed_texts = [preprocess_text(text) for text in text_data]\n",
    "\n",
    "# Create Dictionary and Corpus for LDA\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "# Apply LDA Model\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=20, random_state=100)\n",
    "\n",
    "# Display identified topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5bdbc74",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dictionary.__init__() got an unexpected keyword argument 'num_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(processed_texts, num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Dictionary.__init__() got an unexpected keyword argument 'num_words'"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(processed_texts, num_words=10000)  # Limit vocabulary to 10,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "636f7d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 43521 is out of bounds for axis 1 with size 43521",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train an LDA topic model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mLdaModel(doc_term_matrix, num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, id2word\u001b[38;5;241m=\u001b[39mdictionary, passes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/ldamodel.py:521\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    519\u001b[0m use_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    520\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(corpus, chunks_as_numpy\u001b[38;5;241m=\u001b[39muse_numpy)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    524\u001b[0m     msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    525\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/ldamodel.py:1006\u001b[0m, in \u001b[0;36mLdaModel.update\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: pass \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1004\u001b[0m         pass_, chunk_no \u001b[38;5;241m*\u001b[39m chunksize \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk), lencorpus\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[0;32m-> 1006\u001b[0m     gammat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_estep(chunk, other)\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_alpha:\n\u001b[1;32m   1009\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alpha(gammat, rho())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/ldamodel.py:768\u001b[0m, in \u001b[0;36mLdaModel.do_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[0;32m--> 768\u001b[0m gamma, sstats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(chunk, collect_sstats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    769\u001b[0m state\u001b[38;5;241m.\u001b[39msstats \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sstats\n\u001b[1;32m    770\u001b[0m state\u001b[38;5;241m.\u001b[39mnumdocs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gamma\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# avoids calling len(chunk) on a generator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/ldamodel.py:706\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    704\u001b[0m Elogthetad \u001b[38;5;241m=\u001b[39m Elogtheta[d, :]\n\u001b[1;32m    705\u001b[0m expElogthetad \u001b[38;5;241m=\u001b[39m expElogtheta[d, :]\n\u001b[0;32m--> 706\u001b[0m expElogbetad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpElogbeta[:, ids]\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# phinorm is the normalizer.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[39;00m\n\u001b[1;32m    711\u001b[0m phinorm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(expElogthetad, expElogbetad) \u001b[38;5;241m+\u001b[39m epsilon\n",
      "\u001b[0;31mIndexError\u001b[0m: index 43521 is out of bounds for axis 1 with size 43521"
     ]
    }
   ],
   "source": [
    "# Train an LDA topic model\n",
    "lda_model = models.LdaModel(doc_term_matrix, num_topics=20, id2word=dictionary, passes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc29afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
