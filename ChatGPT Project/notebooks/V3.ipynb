{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ab54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim import corpora, models\n",
    "import re\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "pathway = \"/Users/jeremyfeagan/Library/Mobile Documents/com~apple~CloudDocs/MyGitRepo/ChatGPT Project/conversations.json\"\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(pathway, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80ce954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeremyfeagan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20d1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove hexadecimal codes and non-ASCII characters\n",
    "    text = re.sub(r'\\b[0-9a-fA-F]{4,}\\b', '', text)\n",
    "    # Keep only words with alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1220b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend STOPWORDS with common programming and mathematical terms if necessary\n",
    "extended_stopwords = STOPWORDS.union(set(['=', '+', '-', '*', '/', '(', ')', '#', '->', 'int', 'float', 'print', 'def']))\n",
    "\n",
    "# Initialize a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Extract conversation texts from the 'mapping' key\n",
    "text_data = []\n",
    "for conversation in data:\n",
    "    for key, value in conversation['mapping'].items():\n",
    "        message = value.get('message')\n",
    "        if message:\n",
    "            content = message.get('content')\n",
    "            if content:\n",
    "                parts = content.get('parts')\n",
    "                if parts and isinstance(parts, list):  # Ensure 'parts' is a list\n",
    "                    parts_texts = []\n",
    "                    for part in parts:\n",
    "                        if isinstance(part, dict) and 'text' in part:\n",
    "                            parts_texts.append(part.get('text', ''))\n",
    "                        elif isinstance(part, str):\n",
    "                            parts_texts.append(part)\n",
    "                    combined_text = ' '.join(parts_texts).strip()\n",
    "                    if combined_text:  # Ensure non-empty string\n",
    "                        text_data.append(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090914fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.022*\"item\" + 0.016*\"question\" + 0.014*\"value\" + 0.013*\"correct\" + 0.012*\"answer\"')\n",
      "(1, '0.010*\"provide\" + 0.009*\"time\" + 0.009*\"would\" + 0.009*\"please\" + 0.008*\"like\"')\n",
      "(2, '0.012*\"company\" + 0.009*\"business\" + 0.009*\"agreement\" + 0.007*\"financial\" + 0.006*\"may\"')\n",
      "(3, '0.017*\"file\" + 0.012*\"data\" + 0.010*\"use\" + 0.009*\"code\" + 0.008*\"landing\"')\n",
      "(4, '0.021*\"healthcare\" + 0.017*\"patients\" + 0.016*\"patient\" + 0.013*\"reimbursement\" + 0.013*\"access\"')\n",
      "(5, '0.025*\"data\" + 0.021*\"column\" + 0.013*\"code\" + 0.012*\"import\" + 0.010*\"dataframe\"')\n",
      "(6, '0.008*\"american\" + 0.007*\"literature\" + 0.005*\"world\" + 0.005*\"african\" + 0.004*\"literary\"')\n",
      "(7, '0.024*\"health\" + 0.013*\"data\" + 0.008*\"may\" + 0.007*\"analysis\" + 0.006*\"valuebased\"')\n",
      "(8, '0.015*\"number\" + 0.013*\"year\" + 0.013*\"total\" + 0.011*\"rate\" + 0.010*\"calculate\"')\n",
      "(9, '0.014*\"content\" + 0.014*\"visual\" + 0.011*\"use\" + 0.010*\"text\" + 0.010*\"information\"')\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'text_data' is a list of your conversation texts\n",
    "processed_texts = [preprocess_text(text) for text in text_data]\n",
    "\n",
    "# Create Dictionary and Corpus for LDA\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)  # Limit vocabulary based on word frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "# Apply LDA Model\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=20, random_state=100)\n",
    "\n",
    "# Display identified topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f9d8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.022*\"item\" + 0.016*\"question\" + 0.014*\"value\" + 0.013*\"correct\" + 0.012*\"answer\" + 0.011*\"equation\" + 0.009*\"frac\" + 0.009*\"sentence\" + 0.008*\"find\" + 0.008*\"problem\"\n",
      "Topic 1: 0.010*\"provide\" + 0.009*\"time\" + 0.009*\"would\" + 0.009*\"please\" + 0.008*\"like\" + 0.007*\"questions\" + 0.007*\"make\" + 0.006*\"help\" + 0.006*\"work\" + 0.006*\"information\"\n",
      "Topic 2: 0.012*\"company\" + 0.009*\"business\" + 0.009*\"agreement\" + 0.007*\"financial\" + 0.006*\"may\" + 0.005*\"tax\" + 0.005*\"assets\" + 0.005*\"insurance\" + 0.004*\"information\" + 0.004*\"llc\"\n",
      "Topic 3: 0.017*\"file\" + 0.012*\"data\" + 0.010*\"use\" + 0.009*\"code\" + 0.008*\"landing\" + 0.007*\"python\" + 0.007*\"using\" + 0.007*\"database\" + 0.006*\"files\" + 0.006*\"need\"\n",
      "Topic 4: 0.021*\"healthcare\" + 0.017*\"patients\" + 0.016*\"patient\" + 0.013*\"reimbursement\" + 0.013*\"access\" + 0.011*\"care\" + 0.011*\"providers\" + 0.009*\"specialty\" + 0.009*\"pharmacy\" + 0.007*\"medications\"\n",
      "Topic 5: 0.025*\"data\" + 0.021*\"column\" + 0.013*\"code\" + 0.012*\"import\" + 0.010*\"dataframe\" + 0.009*\"file\" + 0.009*\"str\" + 0.008*\"values\" + 0.008*\"none\" + 0.008*\"columns\"\n",
      "Topic 6: 0.008*\"american\" + 0.007*\"literature\" + 0.005*\"world\" + 0.005*\"african\" + 0.004*\"literary\" + 0.004*\"war\" + 0.004*\"life\" + 0.004*\"new\" + 0.004*\"one\" + 0.003*\"us\"\n",
      "Topic 7: 0.024*\"health\" + 0.013*\"data\" + 0.008*\"may\" + 0.007*\"analysis\" + 0.006*\"valuebased\" + 0.005*\"potential\" + 0.005*\"impact\" + 0.005*\"environmental\" + 0.004*\"research\" + 0.004*\"model\"\n",
      "Topic 8: 0.015*\"number\" + 0.013*\"year\" + 0.013*\"total\" + 0.011*\"rate\" + 0.010*\"calculate\" + 0.009*\"failure\" + 0.009*\"accidents\" + 0.008*\"value\" + 0.008*\"test\" + 0.008*\"fatal\"\n",
      "Topic 9: 0.014*\"content\" + 0.014*\"visual\" + 0.011*\"use\" + 0.010*\"text\" + 0.010*\"information\" + 0.009*\"elements\" + 0.008*\"images\" + 0.007*\"users\" + 0.007*\"color\" + 0.006*\"image\"\n",
      "Topic 0: Programming - 0.022*\"item\" + 0.016*\"question\" + 0.014*\"value\" + 0.013*\"correct\" + 0.012*\"answer\" + 0.011*\"equation\" + 0.009*\"frac\" + 0.009*\"sentence\" + 0.008*\"find\" + 0.008*\"problem\"\n",
      "Topic 1: Business and Finance - 0.010*\"provide\" + 0.009*\"time\" + 0.009*\"would\" + 0.009*\"please\" + 0.008*\"like\" + 0.007*\"questions\" + 0.007*\"make\" + 0.006*\"help\" + 0.006*\"work\" + 0.006*\"information\"\n",
      "Topic 2: Data Analysis - 0.012*\"company\" + 0.009*\"business\" + 0.009*\"agreement\" + 0.007*\"financial\" + 0.006*\"may\" + 0.005*\"tax\" + 0.005*\"assets\" + 0.005*\"insurance\" + 0.004*\"information\" + 0.004*\"llc\"\n",
      "Topic 3: Healthcare - 0.017*\"file\" + 0.012*\"data\" + 0.010*\"use\" + 0.009*\"code\" + 0.008*\"landing\" + 0.007*\"python\" + 0.007*\"using\" + 0.007*\"database\" + 0.006*\"files\" + 0.006*\"need\"\n",
      "Topic 4: Literature - 0.021*\"healthcare\" + 0.017*\"patients\" + 0.016*\"patient\" + 0.013*\"reimbursement\" + 0.013*\"access\" + 0.011*\"care\" + 0.011*\"providers\" + 0.009*\"specialty\" + 0.009*\"pharmacy\" + 0.007*\"medications\"\n",
      "Topic 5: Data Management - 0.025*\"data\" + 0.021*\"column\" + 0.013*\"code\" + 0.012*\"import\" + 0.010*\"dataframe\" + 0.009*\"file\" + 0.009*\"str\" + 0.008*\"values\" + 0.008*\"none\" + 0.008*\"columns\"\n",
      "Topic 6: Arts and Literature - 0.008*\"american\" + 0.007*\"literature\" + 0.005*\"world\" + 0.005*\"african\" + 0.004*\"literary\" + 0.004*\"war\" + 0.004*\"life\" + 0.004*\"new\" + 0.004*\"one\" + 0.003*\"us\"\n",
      "Topic 7: Health and Analysis - 0.024*\"health\" + 0.013*\"data\" + 0.008*\"may\" + 0.007*\"analysis\" + 0.006*\"valuebased\" + 0.005*\"potential\" + 0.005*\"impact\" + 0.005*\"environmental\" + 0.004*\"research\" + 0.004*\"model\"\n",
      "Topic 8: Mathematics - 0.015*\"number\" + 0.013*\"year\" + 0.013*\"total\" + 0.011*\"rate\" + 0.010*\"calculate\" + 0.009*\"failure\" + 0.009*\"accidents\" + 0.008*\"value\" + 0.008*\"test\" + 0.008*\"fatal\"\n",
      "Topic 9: Content and Visualization - 0.014*\"content\" + 0.014*\"visual\" + 0.011*\"use\" + 0.010*\"text\" + 0.010*\"information\" + 0.009*\"elements\" + 0.008*\"images\" + 0.007*\"users\" + 0.007*\"color\" + 0.006*\"image\"\n"
     ]
    }
   ],
   "source": [
    "# Display identified topics and their top words\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic_num, topic in topics:\n",
    "    print(f\"Topic {topic_num}: {topic}\")\n",
    "\n",
    "# Manually label topics based on your domain knowledge\n",
    "topic_labels = {\n",
    "    0: \"Programming\",\n",
    "    1: \"Business and Finance\",\n",
    "    2: \"Data Analysis\",\n",
    "    3: \"Healthcare\",\n",
    "    4: \"Literature\",\n",
    "    5: \"Data Management\",\n",
    "    6: \"Arts and Literature\",\n",
    "    7: \"Health and Analysis\",\n",
    "    8: \"Mathematics\",\n",
    "    9: \"Content and Visualization\"\n",
    "}\n",
    "\n",
    "# Print the labeled topics\n",
    "for topic_num, topic in topics:\n",
    "    print(f\"Topic {topic_num}: {topic_labels[topic_num]} - {topic}\")\n",
    "\n",
    "# Additional refinement can be done based on your manual inspection and domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53525b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Topic 0': '-, +, =, \\\\(, \\\\item, we, 2, can, value, \\\\)',\n",
       " 'Topic 1': 'The, with, as, by, American, on, African, its, that, into',\n",
       " 'Topic 2': 'that, it, as, was, not, be, The, with, this, are',\n",
       " 'Topic 3': 'with, -, healthcare, patient, â€¢, reimbursement, access, that, patients, as',\n",
       " 'Topic 4': '=, #, import, if, ==, 0, 1, str,, as, return',\n",
       " 'Topic 5': 'that, can, health, on, The, with, as, or, are, be',\n",
       " 'Topic 6': 'data, you, that, can, with, The, your, or, as, ```',\n",
       " 'Topic 7': 'I, my, you, your, me, am, have, this, with, that',\n",
       " 'Topic 8': '|, NaN, The, on, its, with, by, financial, will, rate',\n",
       " 'Topic 9': 'your, or, -, you, can, on, with, be, that, are'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Prepare the text data (replace 'text_data' with your dataset)\n",
    "processed_texts = [text.split() for text in text_data]\n",
    "\n",
    "# Create a dictionary and document-term matrix\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)  # Filter out infrequent and very common words\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_texts]\n",
    "\n",
    "# Train an LDA topic model\n",
    "lda_model = models.LdaModel(doc_term_matrix, num_topics=10, id2word=dictionary, passes=15)\n",
    "\n",
    "# Generate topic summaries\n",
    "topic_summaries = {}\n",
    "for topic_id in range(10):\n",
    "    topic_words = lda_model.show_topic(topic_id, topn=10)  # Get the top 10 words for each topic\n",
    "    topic_keywords = [word for word, _ in topic_words]\n",
    "    topic_summary = ', '.join(topic_keywords)\n",
    "    topic_summaries[f'Topic {topic_id}'] = topic_summary\n",
    "\n",
    "topic_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ce259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
