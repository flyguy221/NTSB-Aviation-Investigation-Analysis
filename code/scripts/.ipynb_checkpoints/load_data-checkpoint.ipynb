{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ef6ec9",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "In this section, we will explore the data loading process for the NTSB Accident Dataset. The dataset is sourced from the AVALL.MDB file, which was downloaded from the NTSB Accident Dataset Download Page. To facilitate data analysis, we've set up static URLs for individual tables and manually split the original narratives CSV file into four separate CSV files due to GitHub's 100 MB file size limit. These split files are named as follows: `narr_accp.csv`, `narr_accf.csv`, `narr_cause.csv`, and `narr_inc.csv`.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The data loading process involves several key steps to prepare the dataset for analysis:\n",
    "\n",
    "1. **Loading the Data Dictionary**: The first step is to load the data dictionary CSV file (`eADMSPUB_DataDictionary.csv`) into a DataFrame. This file provides essential information about the dataset, including the names of tables and the data types for each column.\n",
    "\n",
    "2. **Creating a Data Type Dictionary**: Once the data dictionary is loaded, we create a dictionary named `column_data_types`. This dictionary serves as a reference for mapping data types to column names. For each column in the dataset, we extract its data type information from the data dictionary and store it in the `column_data_types` dictionary.\n",
    "\n",
    "3. **Loading Individual Tables**: With the `column_data_types` dictionary prepared, we proceed to load individual tables from the provided URLs. The data types specified in the `column_data_types` dictionary are used during the loading process. We use Pandas' `read_csv` function to read the CSV files from the URLs into DataFrame objects.\n",
    "\n",
    "4. **Merging Narratives Tables**: The dataset includes four separate tables for narratives: `narr_accp`, `narr_accf`, `narr_cause`, and `narr_inc`. If all of these tables are successfully loaded, we merge them into a single narratives DataFrame named `narratives`. This consolidation simplifies narrative analysis.\n",
    "\n",
    "5. **Returning Loaded DataFrames**: Finally, the data loading function returns a dictionary named `loaded_tables` that contains all the loaded DataFrames. These DataFrames are now ready for further analysis and exploration.\n",
    "\n",
    "By following this data loading process, we ensure that the dataset is properly prepared for subsequent data analysis tasks, allowing us to gain valuable insights from the NTSB Accident Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddeb6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.narrative_processing import download_and_process_narratives  # Import the narrative processing function\n",
    "\n",
    "def load_data(load_all=False, **tables):\n",
    "\n",
    "    # Define a dictionary that maps table names to URLs or file paths\n",
    "    table_urls = {\n",
    "        'country': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/Country.csv',\n",
    "        'event_seq': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/Events_Sequence.csv',\n",
    "        'findings': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/Findings.csv',\n",
    "        'flight_crew': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/Flight_Crew.csv',\n",
    "        'ntsb_admin':  'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/NTSB_Admin.csv',\n",
    "        'aircraft': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/aircraft.csv',\n",
    "        'ct_iaids': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/ct_iaids.csv',\n",
    "        'ct_seqevt': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/ct_seqevt.csv',\n",
    "        'dt_flight_crew': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/dt_Flight_Crew.csv',\n",
    "        'dt_aircraft': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/dt_aircraft.csv',\n",
    "        'dt_events': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/dt_events.csv',\n",
    "        'engines': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/engines.csv',\n",
    "        'flight_time': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/flight_time.csv',\n",
    "        'injury': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/injury.csv',\n",
    "        'seq_of_events': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/seq_of_events.csv',\n",
    "        'states': 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/states.csv'\n",
    "\n",
    "    }\n",
    "    \n",
    "     # Load the eADMSPUB_DataDictionary.csv file into a DataFrame\n",
    "    data_dictionary = pd.read_csv('https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/eADMSPUB_DataDictionary.csv')\n",
    "\n",
    "\n",
    "    # Create a dictionary to store column data types\n",
    "    column_data_types = {}\n",
    "\n",
    "    # Data type mapping\n",
    "    data_type_mapping = {\n",
    "        'bit': 'bool',\n",
    "        'char': 'object',\n",
    "        'datetime': 'datetime64[ns]',\n",
    "        'int': 'int64',\n",
    "        'real': 'float64',\n",
    "        'smallint': 'int16',\n",
    "        'text': 'object',\n",
    "        'tinyint': 'int8',\n",
    "        'varchar': 'object',\n",
    "        '(blank)': 'object',\n",
    "    }\n",
    "\n",
    "    # Extract data types from the data dictionary and map to column names\n",
    "    for row in data_dictionary.itertuples(index=False):\n",
    "        table_name = row.Table\n",
    "        column_name = row.Column\n",
    "        data_type = row[6]  # Use the numeric index to access the seventh column\n",
    "        # Map the column name to the corresponding data type\n",
    "        if data_type in data_type_mapping:\n",
    "            column_data_types[f'{table_name}_{column_name}'] = data_type_mapping[data_type]\n",
    "        else:\n",
    "            column_data_types[f'{table_name}_{column_name}'] = 'object'  # Default to 'object' for unknown data types\n",
    "\n",
    "    # Create an empty dictionary to store loaded DataFrames\n",
    "    loaded_tables = {}\n",
    "    \n",
    "    # Special handling for the narratives table\n",
    "    narratives_zip_url = 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/narratives.csv.zip'\n",
    "    extract_path = '/Users/jeremyfeagan/Downloads'\n",
    "    narratives_df = download_and_process_narratives(narratives_zip_url, extract_path)\n",
    "    if not narratives_df.empty:\n",
    "        loaded_tables['narratives'] = narratives_df\n",
    "\n",
    "\n",
    "    # If load_all is True, or if 'narratives' is specifically requested in tables, load the narratives table\n",
    "    if load_all or 'narratives' in tables:\n",
    "        narratives_zip_url = 'https://raw.githubusercontent.com/flyguy221/NTSB-Aviation-Investigation-Analysis/main/data/raw/AVALL/narratives.csv.zip'\n",
    "        extract_path = '/Users/jeremyfeagan/Downloads'  # Update path as needed\n",
    "        narratives_df = download_and_process_narratives(narratives_zip_url, extract_path)\n",
    "        if not narratives_df.empty:\n",
    "            loaded_tables['narratives'] = narratives_df\n",
    "\n",
    "    # Load other tables\n",
    "    for table_name, url in table_urls.items():\n",
    "        if load_all or table_name in tables:\n",
    "            try:\n",
    "                # Adjust dtype based on the data dictionary, if applicable\n",
    "                dtype = column_data_types.get(table_name, None)\n",
    "                loaded_tables[table_name] = pd.read_csv(url, dtype=dtype, low_memory=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading table {table_name}: {str(e)}\")\n",
    "   \n",
    "    return loaded_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52beba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
